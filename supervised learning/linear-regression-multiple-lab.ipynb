{"cells":[{"cell_type":"markdown","metadata":{"id":"github-webhook"},"source":["# Linear Regression with multiple variables\n","\n","Welcome to your second lab! You will build more advanced linear regression algorithm capable of handling any amount of features.\n","\n","You will be predicting house prices given Boston house prices dataset.\n","\n","**Instructions:**\n","- Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\n","\n","**You will learn to:**\n","- Build the general architecture of a learning algorithm, including:\n","    - Initializing parameters\n","    - Calculating the cost function and its gradient\n","    - Using an optimization algorithm (gradient descent)\n","- Gather all three functions above into a main model function, in the right order."]},{"cell_type":"markdown","metadata":{"id":"lZUYyoJbAE4X"},"source":["> **Important note:** Before submission make sure that you **didn't add or delete any notebook cells**. Otherwise your work may not be accepted by the validator!"]},{"cell_type":"markdown","metadata":{"id":"Ool6aydxAE4X"},"source":["## 1 - Packages ##\n","\n","First, let's run the cell below to import all the packages that you will need during this assignment.\n","- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n","- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."]},{"cell_type":"code","execution_count":2,"metadata":{"scrolled":false,"id":"gacSCyn6AE4Y","executionInfo":{"status":"ok","timestamp":1697057706222,"user_tz":-180,"elapsed":284,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"rVxTf5evAE4Z"},"source":["## 2 - Overview of the Dataset ##\n","\n","**Problem Statement**: You are given a dataset  containing:\n","* a training set of `m_train` examples\n","* a test set of `m_test` examples\n","* each example is of shape (number of features, 1)\n","\n","\n","Boston House Prices dataset\n","===========================\n","\n","Notes\n","------\n","Data Set Characteristics:  \n","\n","    :Number of Instances: 506\n","\n","    :Number of Attributes: 13 numeric/categorical predictive\n","    \n","    :Median Value (attribute 14) is usually the target\n","\n","    :Attribute Information (in order):\n","        - CRIM     per capita crime rate by town\n","        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n","        - INDUS    proportion of non-retail business acres per town\n","        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","        - NOX      nitric oxides concentration (parts per 10 million)\n","        - RM       average number of rooms per dwelling\n","        - AGE      proportion of owner-occupied units built prior to 1940\n","        - DIS      weighted distances to five Boston employment centres\n","        - RAD      index of accessibility to radial highways\n","        - TAX      full-value property-tax rate per $10,000\n","    - PTRATIO  pupil-teacher ratio by town\n","    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n","    - LSTAT    % lower status of the population\n","    - MEDV     Median value of owner-occupied homes in $1000's\n","\n","    :Missing Attribute Values: None\n","\n","    :Creator: Harrison, D. and Rubinfeld, D.L.\n","\n","This is a copy of UCI ML housing dataset.\n","https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n","\n","\n","This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n","\n","The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n","prices and the demand for clean air', J. Environ. Economics & Management,\n","vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n","...', Wiley, 1980.   N.B. Various transformations are used in the table on\n","pages 244-261 of the latter.\n","\n","The Boston house-price data has been used in many machine learning papers that address regression\n","problems.   \n","     \n","**References**\n","\n","   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n","   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n","   - many more! (see https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)\n","\n","\n","\n","\n","\n","<b>Let's get more familiar with the dataset. Load the data by running the following code.</b>\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wNHLl5EzAE4a","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1697057708813,"user_tz":-180,"elapsed":1843,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}},"outputId":"b98bfb9f-d9a4-4dc1-e0f2-c50d8ab2c6fa"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-b614e49dba9e>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_set_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualization_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-b614e49dba9e>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \"\"\"\n\u001b[1;32m    155\u001b[0m         )\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Loading the data\n","\n","def load_data():\n","    from sklearn.datasets import load_boston\n","    from sklearn.model_selection import train_test_split\n","\n","    boston = load_boston()\n","\n","    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(boston.data, boston.target, test_size=0.33, random_state=42)\n","\n","    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n","    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n","\n","    return train_set_x.T, train_set_y, test_set_x.T, test_set_y, boston\n","\n","train_set_x, train_set_y, test_set_x, test_set_y, visualization_set = load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-mFOsf--AE4b","executionInfo":{"status":"aborted","timestamp":1697057708814,"user_tz":-180,"elapsed":9,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["print(train_set_x.shape, train_set_y.shape, test_set_x.shape, test_set_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"kXGBV4ReAE4c"},"source":["Many software bugs in machine learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.\n","\n","**Exercise:** Find the values for:\n","* `m_train` (number of training examples)\n","* `m_test` (number of test examples)\n","\n","Remember that `train_set_x` is a numpy-array of shape (number of features, number of examples). For instance, you can access `m_train` by writing `train_set_x.shape[1]`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXLTzPhBAE4c","executionInfo":{"status":"aborted","timestamp":1697057708814,"user_tz":-180,"elapsed":9,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["### START CODE HERE ### (≈ 2 lines of code)\n","m_train = 339\n","m_test = 167\n","### END CODE HERE ###\n","\n","print (\"Number of training examples: m_train = \" + str(m_train))\n","print (\"Number of testing examples: m_test = \" + str(m_test))\n","\n","print (\"\\ntrain_set_x shape: \" + str(train_set_x.shape))\n","print (\"train_set_y shape: \" + str(train_set_y.shape))\n","print (\"test_set_x shape: \" + str(test_set_x.shape))\n","print (\"test_set_y shape: \" + str(test_set_y.shape))"]},{"cell_type":"markdown","metadata":{"id":"kGiVYfqaAE4d"},"source":["**Expected Output for m_train, m_test**:\n","<table style=\"width:15%\">\n","  <tr>\n","      <td><b>m_train</b></td>\n","    <td> 339 </td>\n","  </tr>\n","  \n","  <tr>\n","    <td><b>m_test</b></td>\n","    <td> 167 </td>\n","  </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"HKJEo3oAAE4d"},"source":["### Data visualization"]},{"cell_type":"markdown","metadata":{"id":"XhwCjtqiAE4d"},"source":["Let's plot a histogram of the quantity we want to predict: namely, the house `price`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6doRox9AE4e","executionInfo":{"status":"aborted","timestamp":1697057708814,"user_tz":-180,"elapsed":9,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["plt.figure(figsize=(4, 3))\n","plt.hist(visualization_set.target)\n","plt.xlabel(\"Price ($1000s)\")\n","plt.ylabel(\"Count\")\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"9LfmgswWAE4e"},"source":["And it is very useful to understand the join histogram for each feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjgFBK57AE4e","executionInfo":{"status":"aborted","timestamp":1697057708814,"user_tz":-180,"elapsed":9,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["for index, feature_n  ame in enumerate(visualization_set.feature_names):\n","    plt.figure(figsize=(4, 3))\n","    plt.scatter(visualization_set.data[:, index], visualization_set.target)\n","    plt.ylabel(\"Price\", size=15)\n","    plt.xlabel(feature_name, size=15)\n","    plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"SAoWemulAE4e"},"source":["### Standardization\n","One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array.\n","\n","$$X_{new} = \\frac{X - \\mu}{\\sigma}$$\n","\n","Let's standardize our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a18E_D0bAE4f","executionInfo":{"status":"aborted","timestamp":1697057708814,"user_tz":-180,"elapsed":8,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["all_set_x = np.concatenate([train_set_x, test_set_x], axis=1)\n","\n","mean = all_set_x.mean(axis=1, keepdims=True)\n","std = all_set_x.std(axis=1, keepdims=True)\n","\n","train_set_x = (train_set_x - mean) / std\n","test_set_x = (test_set_x - mean) / std"]},{"cell_type":"markdown","metadata":{"id":"BfksdvoFAE4f"},"source":["## 3 - General Architecture of the learning algorithm ##\n","\n","**Mathematical expression of the algorithm**:\n","\n","\n","For one example $x^{(i)}$:\n","$$h^{(i)} =  w^T x^{(i)} + b \\tag{1}$$\n","\n","The cost is then computed by summing squared diff over all training examples:\n","$$J = \\frac{1}{2m}\\sum_{i=1}^{m}(h^{(i)} - y^{(i)})^{2}\\tag{2}$$\n","\n","**Key steps**:\n","In this exercise, you will carry out the following steps:\n","\n","* Initialize the parameters of the model\n","* Learn the parameters for the model by minimizing the cost  \n","* Use the learned parameters to make predictions (on the test set)\n","* Analyse the results and derive a conclusion"]},{"cell_type":"markdown","metadata":{"id":"M54WqfxEAE4f"},"source":["## 4 - Building the parts of our algorithm ##\n","\n","The main steps for building a learning algorithm:\n","1. Define the model structure (such as number of input features)\n","2. Initialize the model's parameters\n","3. Loop:\n","    - Calculate current loss (forward propagation)\n","    - Calculate current gradient (backward propagation)\n","    - Update parameters (gradient descent)\n","\n","You often build 1-3 separately and integrate them into one function we call `model()`."]},{"cell_type":"markdown","metadata":{"id":"VZmkSQXlAE4f"},"source":["### 4.1 - Initializing parameters\n","\n","**Exercise:** Implement parameter initialization in the cell below. You have to initialize `w` as a vector of zeros. If you don't know what numpy function to use, look up `np.zeros()` in the Numpy library's documentation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7eUCRxcAE4g","executionInfo":{"status":"aborted","timestamp":1697057708814,"user_tz":-180,"elapsed":8,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["# GRADED FUNCTION: initialize_with_zeros\n","\n","def initialize_with_zeros(dim):\n","    \"\"\"\n","    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n","\n","    Argument:\n","    dim -- size of the w vector we want (or number of parameters in this case)\n","\n","    Returns:\n","    w -- initialized vector of shape (dim, 1)\n","    b -- initialized scalar (corresponds to the bias)\n","    \"\"\"\n","\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    w = np.zeros((dim, 1))\n","    b = 0\n","    ### END CODE HERE ###\n","\n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","\n","    return w, b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxmsFwcKAE4g","executionInfo":{"status":"aborted","timestamp":1697057708815,"user_tz":-180,"elapsed":9,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["dim = 2\n","w, b = initialize_with_zeros(dim)\n","print (\"w = \" + str(w))\n","print (\"b = \" + str(b))"]},{"cell_type":"markdown","metadata":{"id":"5t5Eu3NPAE4g"},"source":["**Expected Output**:\n","\n","\n","<table style=\"width:15%\">\n","    <tr>\n","        <td style=\"width:10%\"><b>   w   </b></td>\n","        <td> [[ 0.][ 0.]] </td>\n","    </tr>\n","    <tr>\n","        <td><b>   b   </b></td>\n","        <td> 0 </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"4BZQJqSAAE4g"},"source":["### 4.2 - Forward and Backward propagation\n","\n","Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n","\n","**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.\n","\n","**Hints**:\n","\n","Forward Propagation:\n","- You get X\n","- You compute $H = (w^T X + b) = (h^{(1)}, h^{(2)}, ..., h^{(m-1)}, h^{(m)})$\n","- You calculate the cost function: $J = \\frac{1}{2m}\\sum_{i=1}^{m}(h^{(i)} - y^{(i)})^{2}$\n","\n","\n","Here is the formula of gradient of the cost function:\n","\n","$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(H-Y)^T\\tag{3}$$\n","$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (h^{(i)}-y^{(i)})\\tag{4}$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFJ6rEZYAE4g","executionInfo":{"status":"aborted","timestamp":1697057708815,"user_tz":-180,"elapsed":9,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["def propagate(w, b, X, Y):\n","    \"\"\"\n","    Implement the cost function and its gradient for the propagation explained above\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (number of features, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (number of features, number of examples)\n","    Y -- results of shape (1, number of examples)\n","\n","    Return:\n","    cost -- cost function for linear regression\n","    dw -- gradient of the loss with respect to w, thus same shape as w\n","    db -- gradient of the loss with respect to b, thus same shape as b\n","\n","    Tips:\n","    - Write your code step by step for the propagation.\n","    - Use np.dot() to avoid for-loops in favor of code vectorization\n","    \"\"\"\n","\n","    m = X.shape[1]\n","\n","    # FORWARD PROPAGATION (FROM X TO COST)\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    H = (w.T).dot(X) + b     # compute activation\n","    cost = (1 / (2*m)) * np.sum((H - Y)**2) # compute cost\n","    ### END CODE HERE ###\n","\n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    dw = ((1 / m) * X).dot((H - Y).T)\n","    db = (1 / m) * sum(i for i in (H - Y))\n","    print((np.array(H - Y)))\n","    ### END CODE HERE ###\n","\n","    assert(dw.shape == w.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","\n","    return grads, cost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aq4x2ZTdAE4h","executionInfo":{"status":"aborted","timestamp":1697057708815,"user_tz":-180,"elapsed":9,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n","grads, cost = propagate(w, b, X, Y)\n","print (\"dw = \" + str(grads[\"dw\"]))\n","print (\"db = \" + str(grads[\"db\"]))\n","print (\"cost = \" + str(cost))"]},{"cell_type":"markdown","metadata":{"id":"rqMTomUWAE4h"},"source":["**Expected Output**:\n","\n","<table style=\"width:30%\">\n","    <tr>\n","        <td style=\"width:10%\"><b>   dw   </b></td>\n","      <td> [[ 12.8]\n","     [ 30.82666667]]</td>\n","    </tr>\n","    <tr>\n","        <td><b>   db   </b></td>\n","        <td> 4.533333333333333 </td>\n","    </tr>\n","    <tr>\n","        <td><b>   cost   </b></td>\n","        <td> 41.49333333333333 </td>\n","    </tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"LGFWqYBQAE4h"},"source":["### 4.3 - Optimization\n","- You have initialized your parameters.\n","- You are also able to compute a cost function and its gradient.\n","- Now, you want to update the parameters using gradient descent.\n","\n","**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } \\partial\\theta$, where $\\alpha$ is the learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyMPqgTyAE4h","executionInfo":{"status":"aborted","timestamp":1697057708816,"user_tz":-180,"elapsed":10,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["# GRADED FUNCTION: optimize\n","\n","def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n","    \"\"\"\n","    This function optimizes w and b by running a gradient descent algorithm\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (number of features, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (number of features, number of examples)\n","    Y -- results of shape (1, number of examples)\n","    num_iterations -- number of iterations of the optimization loop\n","    learning_rate -- learning rate of the gradient descent update rule\n","    print_cost -- True to print the loss every 100 steps\n","\n","    Returns:\n","    params -- dictionary containing the weights w and bias b\n","    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n","    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n","\n","    Tips:\n","    You basically need to write down two steps and iterate through them:\n","        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n","        2) Update the parameters using gradient descent rule for w and b.\n","    \"\"\"\n","\n","    costs = []\n","\n","    for i in range(num_iterations):\n","\n","\n","        # Cost and gradient calculation (≈ 1 line of code)\n","        ### START CODE HERE ###\n","        grads, cost = propagate(w, b, X, Y)\n","        ### END CODE HERE ###\n","\n","        # Retrieve derivatives from grads\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","\n","        # update rule (≈ 2 lines of code)\n","        ### START CODE HERE ###\n","        w = w - (learning_rate * dw)\n","        b = b - (learning_rate * db)\n","        ### END CODE HERE ###\n","\n","        # Record the costs\n","        if i % 100 == 0:\n","            costs.append(cost)\n","\n","        # Print the cost every 100 training iterations\n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","\n","    params = {\"w\": w,\n","              \"b\": b}\n","\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","\n","    return params, grads, costs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bA055zCMAE4h","executionInfo":{"status":"aborted","timestamp":1697057708816,"user_tz":-180,"elapsed":10,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n","\n","print (\"w = \" + str(params[\"w\"]))\n","print (\"b = \" + str(params[\"b\"]))\n","print (\"dw = \" + str(grads[\"dw\"]))\n","print (\"db = \" + str(grads[\"db\"]))"]},{"cell_type":"markdown","metadata":{"id":"dbA5TxGXAE4h"},"source":["**Expected Output**:\n","\n","<table style=\"width:35%\">\n","    <tr>\n","        <td style=\"width:10%\"><b>w</b></td>\n","       <td>[[-0.04675219][-0.12676061]] </td>\n","    </tr>\n","    <tr>\n","       <td><b>b</b></td>\n","       <td> 1.223758731602527 </td>\n","    </tr>\n","    <tr>\n","       <td><b>dw</b></td>\n","       <td> [[ 0.12274692]\n","            [-0.09406359]] </td>\n","    </tr>\n","    <tr>\n","       <td><b>db</b></td>\n","       <td> 0.36833971156600487 </td>\n","    </tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"cO12inTsAE4h"},"source":["**Exercise:** The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There is only one step to computing predictions:\n","\n","Calculate $H = w^T X + b$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C87nFQGQAE4h","executionInfo":{"status":"aborted","timestamp":1697057708817,"user_tz":-180,"elapsed":10,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["# GRADED FUNCTION: predict\n","\n","def predict(w, b, X):\n","    \"\"\"\n","    Predict using learned linear regression parameters (w, b)\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (number of features, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (number of features, number of examples)\n","\n","    Returns:\n","    H -- a numpy array (vector) containing all predictions for the examples in X\n","    \"\"\"\n","\n","    m = X.shape[1]\n","\n","    # Compute vector \"H\"\n","    ### START CODE HERE ### (≈ 1 line of code)\n","    H = (w.T).dot(X) + b\n","    ### END CODE HERE ###\n","\n","    assert(H.shape == (1, m))\n","\n","    return H"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nJAllzUAE4i","executionInfo":{"status":"aborted","timestamp":1697057708817,"user_tz":-180,"elapsed":10,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["w = np.array([[0.1124579],[0.23106775]])\n","b = -0.3\n","X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n","print (\"predictions = \" + str(predict(w, b, X)))"]},{"cell_type":"markdown","metadata":{"id":"84pPtVgbAE4i"},"source":["**Expected Output**:\n","\n","<table style=\"width:30%\">\n","    <tr>\n","         <td>\n","             <b>predictions</b>\n","         </td>\n","          <td>\n","            [[ 0.0897392   0.03843181 -0.6367585]]\n","         </td>  \n","   </tr>\n","\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"ZvOGVUlmAE4i"},"source":["<font color='green'>\n","    <b>What to remember:</b>\n","You've implemented several functions that:\n","    \n","- Initialize (w,b)\n","- Optimize the loss iteratively to learn parameters (w,b):\n","    - computing the cost and its gradient\n","    - updating the parameters using gradient descent\n","- Use the learned (w,b) to predict the value for a given set of examples"]},{"cell_type":"markdown","metadata":{"id":"iAqkMoeTAE4i"},"source":["## 5 - Merge all functions into a model ##\n","\n","You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n","\n","**Exercise:** Implement the model function. Use the following notation:\n","\n","* `Y_prediction_test` for your predictions on the test set\n","* `Y_prediction_train` for your predictions on the train set\n","* `w`, `costs`, `grads` for the outputs of optimize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cj_Z2H0pAE4i","executionInfo":{"status":"aborted","timestamp":1697057708817,"user_tz":-180,"elapsed":10,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["# GRADED FUNCTION: model\n","\n","def model(X_train, Y_train, X_test, Y_test, num_iterations=3000, learning_rate=0.5, print_cost=False):\n","    \"\"\"\n","    Builds the linear regression model by calling the function you've implemented previously\n","\n","    Arguments:\n","    X_train -- training set represented by a numpy array of shape (number of features, m_train)\n","    Y_train -- training values represented by a numpy array (vector) of shape (1, m_train)\n","    X_test -- test set represented by a numpy array of shape (number of features, m_test)\n","    Y_test -- test values represented by a numpy array (vector) of shape (1, m_test)\n","    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n","    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n","    print_cost -- Set to true to print the cost every 100 iterations\n","\n","    Returns:\n","    d -- dictionary containing information about the model.\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","\n","    # initialize parameters with zeros (≈ 1 line of code)\n","    w, b = initialize_with_zeros(X_train.shape[0])\n","\n","    # Gradient descent (≈ 1 line of code)\n","    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=False)\n","\n","\n","    # Retrieve parameters w and b from dictionary \"parameters\"\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","\n","    # Predict test/train set examples (≈ 2 lines of code)\n","    Y_prediction_test = predict(w, b, X_test)\n","    Y_prediction_train = predict(w, b, X_train)\n","\n","    ### END CODE HERE ###\n","\n","    # Print train/test Errors\n","    print (\"Train RMSE: {} \".format(np.sqrt(np.mean((Y_prediction_train - Y_train) ** 2))))\n","    print (\"Test RMSE: {} \".format(np.sqrt(np.mean((Y_prediction_test - Y_test) ** 2))))\n","\n","    d = {\"costs\": costs,\n","         \"Y_prediction_test\": Y_prediction_test,\n","         \"Y_prediction_train\" : Y_prediction_train,\n","         \"w\" : w,\n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iterations\": num_iterations}\n","\n","    return d"]},{"cell_type":"markdown","metadata":{"id":"nRBRPAxmAE4i"},"source":["Run the following cell to train your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7csWh8XAE4j","executionInfo":{"status":"aborted","timestamp":1697057708817,"user_tz":-180,"elapsed":10,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=3000, learning_rate=0.05, print_cost=True)"]},{"cell_type":"markdown","metadata":{"id":"_vRMI2gOAE4j"},"source":["**Expected Output**:\n","\n","<table style=\"width:35%\">\n","    <tr>\n","        <td style=\"width:35%\"><b>Cost after iteration 0</b></td>\n","        <td> 307.900929 </td>\n","    </tr>\n","      <tr>\n","        <td> <center> $\\vdots$ </center> </td>\n","        <td> <center> $\\vdots$ </center> </td>\n","    </tr>  \n","    <tr>\n","        <td><b>Train RMSE</b></td>\n","        <td> 4.7941103172540895 </td>\n","    </tr>\n","    <tr>\n","        <td><b>Test RMSE</b></td>\n","        <td> 4.5549106456768715 </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"JXBxqqQdAE4j"},"source":["### Predicted vs True visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dq6mEaUQAE4j","executionInfo":{"status":"aborted","timestamp":1697057708817,"user_tz":-180,"elapsed":10,"user":{"displayName":"Баштовий Іван КА-15","userId":"12442098118245726231"}}},"outputs":[],"source":["# Training set\n","plt.figure(figsize=(4, 3))\n","plt.title(\"Training set\")\n","plt.scatter(train_set_y, d[\"Y_prediction_train\"])\n","plt.plot([0, 50], [0, 50], \"--k\")\n","plt.axis(\"tight\")\n","plt.xlabel(\"True price ($1000s)\")\n","plt.ylabel(\"Predicted price ($1000s)\")\n","plt.tight_layout()\n","\n","# Test set\n","plt.figure(figsize=(4, 3))\n","plt.title(\"Test set\")\n","plt.scatter(test_set_y, d[\"Y_prediction_test\"])\n","plt.plot([0, 50], [0, 50], \"--k\")\n","plt.axis(\"tight\")\n","plt.xlabel(\"True price ($1000s)\")\n","plt.ylabel(\"Predicted price ($1000s)\")\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"vXQLE42bAE4j"},"source":["**Interpretation**:\n","You can see that in fact there is nice linear dependecy between predicted and true values."]},{"cell_type":"markdown","metadata":{"id":"Ds6JwWTdAE4j"},"source":["##### Linear Regression with multiple variables Done!"]},{"cell_type":"markdown","metadata":{"id":"W-oQKOyEAE4j"},"source":["##### Make sure that you didn't add or delete any notebook cells. Otherwise your work may not be accepted by the validator!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}